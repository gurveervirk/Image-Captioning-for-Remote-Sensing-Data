{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, Dataset\n\nt_train_dataset = load_dataset(\"arampacha/rsicd\", split=\"train\")\nt_valid_dataset = load_dataset(\"arampacha/rsicd\", split=\"valid\")\n\nnew_images = []\nnew_captions = []\n\n# Iterate through the original dataset and duplicate rows for each caption\nfor row in t_train_dataset:\n    image = row['image']\n    captions = row['captions']\n    \n    for caption in captions:\n        new_images.append(image)\n        new_captions.append(caption)\n\n# Create a new dataset with the modified data\ntrain_dataset = Dataset.from_dict({'image': new_images, 'captions': new_captions})\n\nnew_images = []\nnew_captions = []\nfor row in t_valid_dataset:\n    image = row['image']\n    captions = row['captions']\n    \n    for caption in captions:\n        new_images.append(image)\n        new_captions.append(caption)\n\n# Create a new dataset with the modified data\nvalid_dataset = Dataset.from_dict({'image': new_images, 'captions': new_captions})","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:21:40.046451Z","iopub.execute_input":"2024-03-09T11:21:40.047292Z","iopub.status.idle":"2024-03-09T11:21:59.660602Z","shell.execute_reply.started":"2024-03-09T11:21:40.047258Z","shell.execute_reply":"2024-03-09T11:21:59.659815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:21:59.662058Z","iopub.execute_input":"2024-03-09T11:21:59.662341Z","iopub.status.idle":"2024-03-09T11:21:59.666943Z","shell.execute_reply.started":"2024-03-09T11:21:59.662316Z","shell.execute_reply":"2024-03-09T11:21:59.665986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport datasets\nimport torch\nfrom transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:21:59.668242Z","iopub.execute_input":"2024-03-09T11:21:59.668574Z","iopub.status.idle":"2024-03-09T11:22:01.339792Z","shell.execute_reply.started":"2024-03-09T11:21:59.668523Z","shell.execute_reply":"2024-03-09T11:22:01.338828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept (LookupError, OSError):\n    nltk.download(\"punkt\", quiet=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:22:01.341847Z","iopub.execute_input":"2024-03-09T11:22:01.342305Z","iopub.status.idle":"2024-03-09T11:22:02.572330Z","shell.execute_reply.started":"2024-03-09T11:22:01.342273Z","shell.execute_reply":"2024-03-09T11:22:02.571570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:22:02.573403Z","iopub.execute_input":"2024-03-09T11:22:02.573899Z","iopub.status.idle":"2024-03-09T11:22:14.001699Z","shell.execute_reply.started":"2024-03-09T11:22:02.573874Z","shell.execute_reply":"2024-03-09T11:22:14.000867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image feature extractor\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n# text tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:22:14.002929Z","iopub.execute_input":"2024-03-09T11:22:14.003210Z","iopub.status.idle":"2024-03-09T11:22:25.986541Z","shell.execute_reply.started":"2024-03-09T11:22:14.003187Z","shell.execute_reply":"2024-03-09T11:22:25.985704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageCapationingDataset(torch.utils.data.Dataset):\n    def __init__(self, ds, max_target_length):\n        self.ds = ds\n        self.max_target_length = max_target_length\n\n    def __getitem__(self, idx):\n        model_inputs = {}\n        image = self.ds[idx][\"image\"]\n        image_encoded = feature_extractor(images=image, return_tensors=\"np\").pixel_values[0]\n        labels = tokenizer(self.ds[idx][\"captions\"], \n                  padding=\"max_length\", truncation=True,\n                  max_length=self.max_target_length).input_ids\n        # This contains image path column\n        model_inputs['labels'] = labels\n        model_inputs['pixel_values'] = image_encoded\n\n        return model_inputs\n\n    def __len__(self):\n        return len(self.ds)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:22:27.998203Z","iopub.execute_input":"2024-03-09T11:22:27.998497Z","iopub.status.idle":"2024-03-09T11:22:28.007456Z","shell.execute_reply.started":"2024-03-09T11:22:27.998456Z","shell.execute_reply":"2024-03-09T11:22:28.006545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = ImageCapationingDataset(train_dataset, 64)\neval_ds = ImageCapationingDataset(valid_dataset, 64)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:30:05.524512Z","iopub.execute_input":"2024-03-09T11:30:05.525394Z","iopub.status.idle":"2024-03-09T11:30:05.529706Z","shell.execute_reply.started":"2024-03-09T11:30:05.525355Z","shell.execute_reply":"2024-03-09T11:30:05.528683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    output_dir=\"./image-captioning-output\",\n    save_total_limit=5,  # You can adjust this based on your requirements\n    learning_rate=5e-5,  # Adjust as needed\n    lr_scheduler_type=\"reduce_lr_on_plateau\",  # Specify the scheduler type\n    save_strategy=\"epoch\",  # You can adjust this based on your requirements\n    num_train_epochs=5,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:44:58.569407Z","iopub.execute_input":"2024-03-09T11:44:58.569784Z","iopub.status.idle":"2024-03-09T11:44:58.579491Z","shell.execute_reply.started":"2024-03-09T11:44:58.569753Z","shell.execute_reply":"2024-03-09T11:44:58.578578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q evaluate rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:23:00.838574Z","iopub.status.idle":"2024-03-09T11:23:17.005765Z","shell.execute_reply.started":"2024-03-09T11:23:00.839215Z","shell.execute_reply":"2024-03-09T11:23:17.004501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nmetric = evaluate.load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:23:32.195462Z","iopub.execute_input":"2024-03-09T11:23:32.195826Z","iopub.status.idle":"2024-03-09T11:23:32.724953Z","shell.execute_reply.started":"2024-03-09T11:23:32.195796Z","shell.execute_reply":"2024-03-09T11:23:32.724160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nignore_pad_token_for_loss = True\n\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    if ignore_pad_token_for_loss:\n        # Replace -100 in the labels as we can't decode them.\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n                                                     decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds,\n                            references=decoded_labels,\n                            use_stemmer=True)\n    result = {k: round(v * 100, 4) for k, v in result.items()}\n    prediction_lens = [\n        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n    ]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:23:35.152501Z","iopub.execute_input":"2024-03-09T11:23:35.153335Z","iopub.status.idle":"2024-03-09T11:23:35.163179Z","shell.execute_reply.started":"2024-03-09T11:23:35.153301Z","shell.execute_reply":"2024-03-09T11:23:35.162049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_scheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Define the AdamW optimizer\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n\n# Define the ReduceLROnPlateau scheduler\nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.1,\n    patience=3,\n    verbose=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:30:14.396777Z","iopub.execute_input":"2024-03-09T11:30:14.397514Z","iopub.status.idle":"2024-03-09T11:30:14.407594Z","shell.execute_reply.started":"2024-03-09T11:30:14.397482Z","shell.execute_reply":"2024-03-09T11:30:14.406708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import default_data_collator\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=feature_extractor,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    data_collator=default_data_collator,\n    optimizers=(optimizer, scheduler),\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:45:02.968568Z","iopub.execute_input":"2024-03-09T11:45:02.968930Z","iopub.status.idle":"2024-03-09T11:45:02.988066Z","shell.execute_reply.started":"2024-03-09T11:45:02.968899Z","shell.execute_reply":"2024-03-09T11:45:02.987270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:45:04.064913Z","iopub.execute_input":"2024-03-09T11:45:04.065529Z","iopub.status.idle":"2024-03-09T11:45:12.763846Z","shell.execute_reply.started":"2024-03-09T11:45:04.065498Z","shell.execute_reply":"2024-03-09T11:45:12.762518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom nltk.translate.bleu_score import sentence_bleu\n\nres = []\nmax_length = 64\nnum_beams = 4\ngen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\nfor i, example in enumerate(t_valid_dataset):\n    reference = [ caption.split() for caption in example[\"captions\"] ] \n    image = example[\"image\"]\n    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n    pixel_values = pixel_values.to(\"cuda\")\n\n    output_ids = model.generate(pixel_values, **gen_kwargs)\n\n    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n    preds = [pred.strip() for pred in preds][0]\n    res.append(sentence_bleu(reference, preds.split()))\n\nprint(sum(res) / len(res))","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:42:24.569432Z","iopub.execute_input":"2024-03-09T11:42:24.570252Z","iopub.status.idle":"2024-03-09T11:42:59.882500Z","shell.execute_reply.started":"2024-03-09T11:42:24.570210Z","shell.execute_reply":"2024-03-09T11:42:59.881627Z"},"trusted":true},"execution_count":null,"outputs":[]}]}